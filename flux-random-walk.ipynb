{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxpAz7VJR0P"
      },
      "source": [
        "##Flux random walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi4rbBNcNjVu"
      },
      "outputs": [],
      "source": [
        "# This is a simple python notebook to render random walks through flux latent\n",
        "# space, simultaneously traversing prompt embed space (concepts) and init noise\n",
        "# space (shapes).\n",
        "#\n",
        "# Works with flux-schnell-fp8 and flux-dev-fp8. Requires less than 12 GB RAM\n",
        "# and less than 15 GB VRAM, so it should run fine on Google Colab. Faster with\n",
        "# a better GPU, of course.\n",
        "#\n",
        "# Robert Luxemburg, 2024, Public Domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj0xf8QJkod6"
      },
      "source": [
        "##Create output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0JtTu4PJ8f0"
      },
      "outputs": [],
      "source": [
        "# Once you're certain that this notebook does not mess with your personal\n",
        "# files, set trust_this_notebook to True. This will allow it to save the\n",
        "# generated images and videos directly to your Google Drive.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "trust_this_notebook = False\n",
        "\n",
        "if trust_this_notebook:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/FLUX.1/outputs\"\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/outputs\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w1r4707lUEH"
      },
      "source": [
        "##Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs7OGDYpKXbU"
      },
      "outputs": [],
      "source": [
        "# This code is based on camenduru's flux notebook. You can find the original at\n",
        "# https://github.com/camenduru/flux-jupyter/blob/main/flux.1-dev_jupyter.ipynb\n",
        "#\n",
        "# The flux model can be selected below. Schnell is schneller, dev looks better.\n",
        "#\n",
        "# If you're not planning to use \"tokens\" mode (see further down), you can set\n",
        "# load_encoder to False. This will save some time on startup, and some memory.\n",
        "\n",
        "model = \"schnell\" # \"schnell\" or \"dev\"\n",
        "load_encoder = True\n",
        "\n",
        "%cd /content\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "# drill a tiny hole through some comfy internals\n",
        "filename = \"/content/TotoroUI/totoro/sample.py\"\n",
        "source = open(filename).read()\n",
        "string = \"generator = torch.manual_seed(seed)\"\n",
        "patch = \"if type(seed) is torch.Tensor: return seed\\n    \"\n",
        "open(filename, \"w\").write(source.replace(string, patch + string))\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.27\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "if load_encoder:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.sft\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.sft\n",
        "\n",
        "if model == \"schnell\":\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/kijai/flux-fp8/resolve/main/flux1-schnell-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-schnell-fp8.sft\n",
        "else:\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/kijai/flux-fp8/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn9OfyY5mCfS"
      },
      "source": [
        "##Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ21zVMFiFZN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro import model_management\n",
        "from totoro_extras import nodes_custom_sampler, nodes_flux\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    if load_encoder:\n",
        "        clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.sft\", \"clip_l.sft\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(f\"flux1-{model}-fp8.sft\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltHNuFg4mvM8"
      },
      "source": [
        "##Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icgA5fusO_fX"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "def get_noise(seed, shape, sigma):\n",
        "    g = torch.Generator().manual_seed(seed) if type(seed) is int else seed\n",
        "    noise = torch.randn(shape, generator=g)\n",
        "    sigmas = [sigma] + (len(shape) - 1) * [0]\n",
        "    noise = gaussian_filter(noise, sigma=sigmas, mode=\"wrap\")\n",
        "    noise = (noise - noise.mean()) / noise.std()\n",
        "    return torch.Tensor(noise).to(torch.float16)\n",
        "\n",
        "def get_tokens(seed):\n",
        "    g = torch.Generator().manual_seed(seed) if type(seed) is int else seed\n",
        "    ids_l = [49406] + torch.randint(1, 49406, (75,), generator=g).tolist() + [49407]\n",
        "    ids_t5xxl = torch.randint(2, 32128, (255,), generator=g).tolist() + [1]\n",
        "    return {\n",
        "        \"l\": [[(id, 1.0) for id in ids_l]],\n",
        "        \"t5xxl\": [[(id, 1.0) for id in ids_t5xxl]]\n",
        "    }\n",
        "\n",
        "def slerp(vs, t, loop=True, DOT_THRESHOLD=0.9995):\n",
        "    n = len(vs)\n",
        "    if n == 1:\n",
        "        return vs[0]\n",
        "    nn = n if loop else n - 1\n",
        "    v0 = vs[int(t * nn) % n]\n",
        "    v1 = vs[int(t * nn + 1) % n]\n",
        "    t = t * nn % 1\n",
        "    dot = torch.sum(v0 * v1 / (torch.linalg.norm(v0) * torch.linalg.norm(v1)))\n",
        "    if torch.abs(dot) > DOT_THRESHOLD or torch.isnan(dot):\n",
        "        return (1 - t) * v0 + t * v1\n",
        "    theta_0 = torch.acos(dot)\n",
        "    sin_theta_0 = torch.sin(theta_0)\n",
        "    theta_t = theta_0 * t\n",
        "    sin_theta_t = torch.sin(theta_t)\n",
        "    s0 = torch.sin(theta_0 - theta_t) / sin_theta_0\n",
        "    s1 = sin_theta_t / sin_theta_0\n",
        "    return s0 * v0 + s1 * v1\n",
        "\n",
        "def encode(tokens):\n",
        "    with torch.inference_mode():\n",
        "        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n",
        "    return cond, pooled\n",
        "\n",
        "def render(\n",
        "    filename,\n",
        "    prompt, # tuple of tensors ((1, 256, 4096), (1, 768))\n",
        "    noise,  # tensor (16, height//8, width//8)\n",
        "    steps=4 if model == \"schnell\" else 20,\n",
        "    guidance=3.5\n",
        "):\n",
        "    if os.path.exists(filename):\n",
        "        return\n",
        "    print(filename.replace(f\"{OUTPUT_DIR}/\", \"\"))\n",
        "    cond = [[prompt[0], {\"pooled_output\": prompt[1]}]]\n",
        "    width, height = noise.shape[2] * 8, noise.shape[1] * 8\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        cond = FluxGuidance.append(cond, guidance)[0]\n",
        "        random_noise = RandomNoise.get_noise(noise)[0]\n",
        "        guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "        sampler = KSamplerSelect.get_sampler(\"euler\")[0]\n",
        "        sigmas = BasicScheduler.get_sigmas(unet, \"simple\", steps, 1.0)[0]\n",
        "        latent_image = EmptyLatentImage.generate(width, height)[0]\n",
        "        sample, sample_denoised = SamplerCustomAdvanced.sample(\n",
        "            random_noise, guider, sampler, sigmas, latent_image\n",
        "        )\n",
        "        model_management.soft_empty_cache()\n",
        "        decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "        image = Image.fromarray(np.array(decoded * 255, dtype=np.uint8)[0])\n",
        "\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    image.save(filename)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDbs4l0Jm1Rr"
      },
      "source": [
        "##Render images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kmzy8oxY1Yc"
      },
      "outputs": [],
      "source": [
        "# There are three different modes, \"gauss\", \"slerp\" and \"tokens\".\n",
        "#\n",
        "# \"gauss\" applies gaussian blur to the input noise, resulting in smoother\n",
        "# transitions between frames. For long sequences, this will run out of memory.\n",
        "# With plot_noise = True, you can check how the input looks for a given sigma.\n",
        "#\n",
        "# \"slerp\" performs spherial linear interpolation along a sequence of random\n",
        "# samples. This uses less RAM, but results in faster transitions and can lead\n",
        "# to visible changes of direction (if you know what you're looking for).\n",
        "#\n",
        "# \"tokens\" works like \"slerp\", but instead of random noise uses embeddings of\n",
        "# random tokens. This will take some time on startup, but should produce fewer\n",
        "# blank, blurry or monochromatic images (i.e. totally \"meaningless\" noise).\n",
        "#\n",
        "# You can set mode, width, height, steps, sigma and seed below. Width and\n",
        "# height must be multiples of 16, steps are the number of frames, and sigma is\n",
        "# the amount of blur (\"gauss\") or the number of samples (\"slerp\" or \"tokens\").\n",
        "#\n",
        "# This renders two consecutive frames first, so you can estimate the rate of\n",
        "# change, and then a few \"keyframes\". If you don't like them, pick a new seed.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "mode = \"gauss\" # \"gauss\", \"slerp\" or \"tokens\"\n",
        "width, height = 1024, 1024\n",
        "steps, sigma = 225, 15\n",
        "seed = 42\n",
        "plot_noise = True\n",
        "\n",
        "dirname = f\"{model},{mode},{width},{height},{steps},{sigma},{seed}\"\n",
        "g = torch.Generator().manual_seed(seed)\n",
        "\n",
        "if mode == \"gauss\":\n",
        "    cond = get_noise(g, (steps, 1, 256, 4096), sigma)\n",
        "    pooled = get_noise(g, (steps, 1, 768), sigma)\n",
        "    init = get_noise(g, (steps, 16, height//8, width//8), sigma)\n",
        "elif mode == \"slerp\":\n",
        "    cond = torch.randn((sigma, 1, 256, 4096), generator=g)\n",
        "    pooled = torch.randn((sigma, 1, 768), generator=g)\n",
        "    init = torch.randn((sigma, 16, height//8, width//8), generator=g)\n",
        "else:\n",
        "    cond_pooled = (encode(get_tokens(g)) for _ in tqdm(range(sigma)))\n",
        "    cond, pooled = zip(*cond_pooled)\n",
        "    init = torch.randn((sigma, 16, height//8, width//8), generator=g)\n",
        "\n",
        "# trying to match mean and std of T5+Clip encoder output\n",
        "if mode in (\"gauss\", \"slerp\"):\n",
        "    cond *= 0.14\n",
        "    pooled -= 0.11\n",
        "\n",
        "if plot_noise:\n",
        "    if mode == \"gauss\":\n",
        "        plt.plot(init[:,0,0,0])\n",
        "    else:\n",
        "        plt.plot([slerp(init[:,0,0,0], t) for t in np.arange(0, 1, 1 / steps)])\n",
        "    plt.show()\n",
        "\n",
        "n = steps//sigma if mode == \"gauss\" else sigma\n",
        "for step in (steps - 1, n, 1):\n",
        "    for i in range(0, steps, step):\n",
        "        if mode == \"gauss\":\n",
        "            prompt = cond[i], pooled[i]\n",
        "            noise = init[i]\n",
        "        else:\n",
        "            t = i / steps\n",
        "            prompt = slerp(cond, t), slerp(pooled, t)\n",
        "            noise = slerp(init, t)\n",
        "        render(\n",
        "            f\"{OUTPUT_DIR}/random_walk/{dirname}/{i:08d}.png\",\n",
        "            prompt,\n",
        "            noise\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOMeTkSdssLC"
      },
      "source": [
        "##Render video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDczvu6Qsq5k"
      },
      "outputs": [],
      "source": [
        "# For the video output, you can chose a start frame and the direction (1 for\n",
        "# forward, -1 for reversed). You may also want to upscale the video slightly,\n",
        "# to keep sites like YouTube from downscaling it later. fps is the frame rate.\n",
        "\n",
        "start = 0\n",
        "direction = 1 # 1 or -1\n",
        "width, height = 1080, 1080\n",
        "fps = 15\n",
        "\n",
        "path = f\"{OUTPUT_DIR}/random_walk/{dirname}\"\n",
        "filename = f\"{path}.txt\"\n",
        "indices = ((start + i * direction) % steps for i in range(steps))\n",
        "frames = (f\"file '{path}/{i:08d}.png'\" for i in indices)\n",
        "open(filename, \"w\").write(\"\\n\".join(frames))\n",
        "\n",
        "!ffmpeg -y -r {fps} -f concat -safe 0 -i {filename} \\\n",
        "    -vf scale={width}:{height}:flags=lanczos \\\n",
        "    -vcodec libx264 -pix_fmt yuv420p -crf 17 {path}.mp4\n",
        "\n",
        "os.remove(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
